{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"flux_",
				"flux_keys"
			],
			[
				"log",
				"log10"
			],
			[
				"sb_ef",
				"sb_eff_ary"
			],
			[
				"sb_eff",
				"sb_eff_mat"
			],
			[
				"g2_r",
				"g2_ref_mat"
			],
			[
				"x_m_",
				"x_m_sum_f"
			],
			[
				"inc_",
				"incgamma_fct_p"
			],
			[
				"g2_ref",
				"g2_ref_ary"
			],
			[
				"g1_ref",
				"g1_ref_mat"
			],
			[
				"k_m",
				"k_max_ary"
			],
			[
				"dou",
				"double_array_template"
			],
			[
				"new",
				"new_x"
			],
			[
				"k",
				"k"
			],
			[
				"g1_re",
				"g1_ref_ary"
			],
			[
				"g1_r",
				"g1_ref_ary"
			],
			[
				"g2_",
				"g2_ary_f"
			],
			[
				"g2",
				"g2_ref_mat"
			],
			[
				"g",
				"g1_ref_mat"
			],
			[
				"g1_",
				"g1_mat_f"
			],
			[
				"g1",
				"g1_mat_f"
			],
			[
				"DTE",
				"DTYPEf_t"
			],
			[
				"x",
				"x"
			],
			[
				"interp",
				"interp1d"
			],
			[
				"kmax",
				"kmax_ary"
			],
			[
				"np",
				"npix_roi"
			],
			[
				"g2_ar",
				"g2_ary_k"
			],
			[
				"incg",
				"incgamma_lo_fct_ary"
			],
			[
				"templ",
				"template_masked_compressed_expreg"
			],
			[
				"flux",
				"flux_maps_dict_nested"
			],
			[
				"fl",
				"flux_map"
			],
			[
				"tem",
				"templates_dict"
			],
			[
				"return",
				"return_masked_compressed"
			],
			[
				"masked",
				"masked_array"
			],
			[
				"c",
				"compressed"
			],
			[
				"flu",
				"flux_template"
			],
			[
				"el",
				"elif"
			],
			[
				"conce",
				"concentration_model"
			],
			[
				"l",
				"log"
			],
			[
				"rho",
				"rho_s"
			]
		]
	},
	"buffers":
	[
		{
			"file": "npll.pyx",
			"settings":
			{
				"buffer_size": 4260,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "pdf_sampler.py",
			"settings":
			{
				"buffer_size": 1639,
				"line_ending": "Unix"
			}
		},
		{
			"file": "nptf_scan.py",
			"settings":
			{
				"buffer_size": 30507,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "config_maps.py",
			"settings":
			{
				"buffer_size": 12281,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "interp1d.pyx",
			"settings":
			{
				"buffer_size": 6351,
				"encoding": "UTF-8",
				"line_ending": "Unix",
				"name": "import numpy as N"
			}
		},
		{
			"contents": "###############################################################################\n# x_m.pyx\n###############################################################################\n#\n# Calculates x_m and x_m_sum arrays for arbitary number of breaks.\n# Uses dedicated functions for 1, 2 and 3 breaks as well as n > 3 breaks.\n#\n###############################################################################\n\nimport numpy as np\ncimport numpy as np\ncimport cython\nfrom cpython cimport bool\n\n# from libc.math cimport fabs\n\ntry:\n    import incgamma_fct as igf\n    print(\"Using C x_m\")\nexcept ImportError:\n    import incgamma_fct_p as igf\n    print(\"Using Python x_m\")\n\nimport interp1d as interp\nimport findmin\n\n# Type used for all non-integer functions\nDTYPE = np.float\n\n# Setup cython functions used\ncdef extern from \"math.h\":\n    double pow(double x, double y) nogil\n\n\ndef return_xs(double[::1] theta, double[::1] f_ary, double[::1] df_rho_div_f_ary, \n              double[::1] ft_compressed, double[::1] npt_compressed, int[::1] data):\n    \"\"\" Returns arrays of x_m and x_m_sum for likelihood calculations\n\n        :param theta: Array of parameters, [A, n[1], .., n[j+1], Sb[1], .., Sb[j]]\n        :param f_ary: Photon leakage probabilities characterizing PSF, sum(f_ary) = 1.0\n        :param df_rho_div_f_ary: df*rho(f)/f for integrating over f as a sum\n        :param ft_compressed: Pixel-wise flux template. For a nontrivial flux template, scaling\n        flux by a factor of c corresponds to scaling normalization A by 1/c and breaks Sb[j] by c.\n        :param npt_compressed: Pixel-wise normalization of the PS template\n        :param data: The pixel-wise data\n        :returns: A list containing (x_m, x_m_sum)\n    \"\"\"\n\n    cdef int n_break = int((len(theta) - 2)/2) \n\n    if n_break == 1:\n        return return_xs_1break(theta, f_ary, df_rho_div_f_ary, ft_compressed, npt_compressed,\n                                data)\n    elif n_break == 2:\n        # return return_xs_2break_orig(theta, f_ary, df_rho_div_f_ary, npt_compressed, data)\n        return return_xs_2break(theta, f_ary, df_rho_div_f_ary, ft_compressed, npt_compressed,\n                                data)\n    elif n_break == 3:\n        return return_xs_3break(theta, f_ary, df_rho_div_f_ary, ft_compressed, npt_compressed,\n                                data)\n    else:\n        return return_xs_lbreak(theta, n_break, f_ary, df_rho_div_f_ary, \n                                npt_compressed, data)\n        print(\"Sorry, flux template not yet implemented!\")\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.cdivision(True)\n@cython.initializedcheck(False)\ndef return_xs_1break(double[::1] theta, double[::1] f_ary, double[::1] df_rho_div_f_ary, \n                     double[::1] ft_compressed, double[::1] npt_compressed, int[::1] data):\n    \"\"\" Dedicated calculation of x_m and x_m_sum for 1 break \n    \"\"\"\n\n    cdef double a_ps = float(theta[0])\n    cdef double n1 = float(theta[1])\n    cdef double n2 = float(theta[2])\n    cdef double sb = float(theta[3])\n\n    cdef int k_max = int(max(data) + 1)\n    cdef int npix_roi = len(npt_compressed)\n\n    cdef double[:,::1] x_m_ary = np.zeros((npix_roi,k_max + 1), dtype=DTYPE)\n    cdef double[::1] x_m_sum = np.zeros(npix_roi, dtype=DTYPE)\n\n    cdef double[::1] g1_ary_f = np.empty(k_max + 1, dtype=DTYPE)\n    cdef double[::1] g2_ary_f = np.empty(k_max + 1, dtype=DTYPE)\n\n    cdef double f2, df_rho_div_f2\n    cdef double pref1_x_m_ary, pref2_x_m_ary\n    cdef double x_m_sum_f, x_m_ary_f\n    cdef double minval1, minval2\n\n    cdef Py_ssize_t f_index, p, k, i\n\n    cdef double[:] sb_ref_ary = sb*np.logspace(-1, 1, 11, dtype=DTYPE)\n\n    cdef double[:,::1] g1_ref_mat = np.empty((len(sb_ref_ary), k_max + 1), dtype=DTYPE)\n    cdef double[:,::1] g2_ref_mat = np.empty((len(sb_ref_ary), k_max + 1), dtype=DTYPE)\n\n    cdef double[:,::1] g1_mat_f = np.empty((npix_roi, k_max+1), dtype=DTYPE)\n    cdef double[:,::1] g2_mat_f = np.empty((npix_roi, k_max+1), dtype=DTYPE)\n\n    cdef double[:] g1_ref_ary = np.empty(len(sb_ref_ary), dtype=DTYPE)\n    cdef double[:] g2_ref_ary = np.empty(len(sb_ref_ary), dtype=DTYPE)\n\n    cdef double[:] g1_int_ary = np.empty(npix_roi, dtype=DTYPE)\n    cdef double[:] g2_int_ary = np.empty(npix_roi, dtype=DTYPE)\n\n    cdef double[::1] ffac_ary = np.empty(npix_roi, dtype=DTYPE)\n    cdef double[:] sb_eff_ary = np.empty (npix_roi, dtype=DTYPE)\n    cdef double[:,::1] sb_eff_mat = np.zeros((npix_roi, k_max+1), dtype=DTYPE)\n\n    cdef bool ft_cond = False\n\n    if len(ft_compressed) != 0:\n        ft_cond = True\n        # print(\"Nontrivial flux template provided\")\n        # assert(0 not in ft_compressed), \\\n        #     \"Flux template contains one or more 0's!\"       \n    # if ft_cond != True:\n    #     print(\"No nontrivial flux templates\")\n\n    for f_index in range(len(f_ary)):\n        f2 = float(f_ary[f_index])\n        df_rho_div_f2 = df_rho_div_f_ary[f_index]\n\n        # For the case of a nontrivial flux template, generate matrix of incomplete gamma function values\n        # to interpolate about, and interpolate appropriately for each pixel\n        if ft_cond:\n            for i in range(len(sb_ref_ary)):\n                g1_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n1, sb_ref_ary[i] * f2)\n                g1_ref_mat[i,:] = g1_ary_f\n                g2_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n2, sb_ref_ary[i] * f2)\n                g2_ref_mat[i,:] = g2_ary_f\n            \n            for p in range(npix_roi):\n                for k in range(k_max):\n                    if k <= data[p]+1:\n                        sb_eff_mat[p,k] = ft_compressed[p] * sb \n\n            for k in range(k_max):\n                g1_ref_ary = g1_ref_mat[:,k]\n                g2_ref_ary = g2_ref_mat[:,k]\n                sb_eff_ary = sb_eff_mat[:,k]\n                # print(\"sb_eff_ary = \", sb_eff_ary)\n\n                minval1 = findmin.minval(g1_ref_ary)\n                if minval1 <= 0:\n                    g1_int_ary = interp.logloginterp1d(sb_ref_ary, g1_ref_ary, sb_eff_ary, offset = 1-minval1) \n                else:\n                    g1_int_ary = interp.logloginterp1d(sb_ref_ary, g1_ref_ary, sb_eff_ary)\n                g1_mat_f[:,k] = g1_int_ary\n                # print(\"g1_int_ary = \", g1_int_ary)\n\n                minval2 = findmin.minval(g2_ref_ary)\n                if minval2 <= 0:\n                    g2_int_ary = interp.logloginterp1d(sb_ref_ary, g2_ref_ary, sb_eff_ary, offset = 1-minval2) \n                else:\n                    g2_int_ary = interp.logloginterp1d(sb_ref_ary, g2_ref_ary, sb_eff_ary)\n                g2_mat_f[:,k] = g2_int_ary\n                # print(\"g2_int_ary = \", np.asarray(g2_int_ary))\n\n\n            for p in range(npix_roi):\n                a_ps = float(theta[0])/ft_compressed[p]\n                sb = ft_compressed[p] * float(theta[3])\n\n                pref1_x_m_ary =  pow(sb * f2, n1)\n                pref2_x_m_ary = pow(sb * f2, n2)\n\n                x_m_sum_f = a_ps*sb*f2*(1/(n1-1)+1/(1-n2)) * npt_compressed[p]\n                x_m_sum[p] += df_rho_div_f2*x_m_sum_f\n\n                for k in range(data[p]+1):\n                    # print('g1=', g1_mat_f[p])\n                    # print('g2=',g2_mat_f[p])\n                    x_m_ary_f = a_ps * (pref1_x_m_ary*g1_mat_f[p,k] + pref2_x_m_ary*g2_mat_f[p,k]) * npt_compressed[p]\n                    x_m_ary[p,k] += df_rho_div_f2*x_m_ary_f\n\n        else:\n            g1_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n1, sb * f2)\n            g2_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n2, sb * f2)\n\n            for p in range(npix_roi):\n                pref1_x_m_ary =  pow(sb * f2, n1)\n                pref2_x_m_ary = pow(sb * f2, n2)\n\n                x_m_sum_f = a_ps*sb*f2*(1/(n1-1)+1/(1-n2)) * npt_compressed[p]\n                x_m_sum[p] += df_rho_div_f2*x_m_sum_f\n\n                for k in range(data[p]+1):\n                    x_m_ary_f = a_ps * (pref1_x_m_ary*g1_ary_f[k] + pref2_x_m_ary*g2_ary_f[k]) * npt_compressed[p]\n                    x_m_ary[p,k] += df_rho_div_f2*x_m_ary_f\n            \n    x_m_sum = np.asarray(x_m_sum) - np.asarray(x_m_ary)[:,0] \n\n    # return np.asarray(g1_ref_mat), np.asarray(g2_ref_mat)\n    return np.asarray(x_m_ary), np.asarray(x_m_sum)\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.cdivision(True)\n@cython.initializedcheck(False)\ndef return_xs_2break(double[::1] theta, double[::1] f_ary, \n                     double[::1] df_rho_div_f_ary, double[::1] ft_compressed, double[::1] npt_compressed,\n                     int[::1] data):\n    \"\"\" Dedicated calculation of x_m and x_m_sum for 2 breaks \n    \"\"\"\n\n    cdef float a_ps = float(theta[0])\n    cdef float n1 = float(theta[1])\n    cdef float n2 = float(theta[2])\n    cdef float n3 = float(theta[3])\n    cdef float sb1 = float(theta[4])\n    cdef float sb2 = float(theta[5])\n\n    cdef int k_max = int(np.max(data) + 1)\n    cdef int npix_roi = len(npt_compressed)\n\n    cdef double[:,::1] x_m_ary = np.zeros(shape=(npix_roi,k_max + 1))\n    cdef double[::1] x_m_sum = np.zeros(npix_roi)\n\n    cdef double[::1] g0_ary_f = np.zeros(k_max + 1)\n    cdef double[::1] g1_ary_f = np.zeros(k_max + 1)\n    cdef double[::1] g1_ary_f_1 = np.zeros(k_max + 1)\n    cdef double[::1] g1_ary_f_2 = np.zeros(k_max + 1)\n    cdef double[::1] g2_ary_f = np.zeros(k_max + 1)\n\n    cdef double f2, df_rho_div_f2\n    cdef double pref0_x_m_ary, pref1_x_m_ary, pref2_x_m_ary\n    cdef double first0_x_m_sum_ary, first1_x_m_sum_ary, first2_x_m_sum_ary\n    cdef double second0_x_m_sum_ary, second1_x_m_sum_ary, second2_x_m_sum_ary\n    cdef double x_m_sum_f, x_m_ary_f\n    cdef double minval0, minval1, minval2\n\n    cdef Py_ssize_t f_index, p, k, i, j\n\n    first0_x_m_sum_ary = 1/(1-n1)\n    first1_x_m_sum_ary = 1/(1-n2)\n    first2_x_m_sum_ary = 1/(1-n3)\n \n    second0_x_m_sum_ary = -1.0\n    second1_x_m_sum_ary = (1 - pow(sb2/sb1, 1-n2))\n    second2_x_m_sum_ary = pow(sb2/sb1, 1-n2)\n\n    cdef double[:] sb1_ref_ary = sb1*np.logspace(-1, 1, 11, dtype=DTYPE)\n    cdef double[:] sb2_ref_ary = sb2*np.logspace(-1, 1, 11, dtype=DTYPE)\n\n    cdef double[:,::1] g0_ref_mat = np.empty((len(sb1_ref_ary), k_max + 1), dtype=DTYPE)\n    cdef double[:,::1] g1_ref_mat_1 = np.empty((len(sb1_ref_ary), k_max + 1), dtype=DTYPE)\n    cdef double[:,::1] g1_ref_mat_2 = np.empty((len(sb2_ref_ary), k_max + 1), dtype=DTYPE)\n    cdef double[:,::1] g2_ref_mat = np.empty((len(sb2_ref_ary), k_max + 1), dtype=DTYPE)\n\n    cdef double[:,::1] g0_mat_f = np.empty((npix_roi, k_max+1), dtype=DTYPE)    \n    cdef double[:,::1] g1_mat_f_1 = np.empty((npix_roi, k_max+1), dtype=DTYPE)\n    cdef double[:,::1] g1_mat_f_2 = np.empty((npix_roi, k_max+1), dtype=DTYPE)\n    cdef double[:,::1] g2_mat_f = np.empty((npix_roi, k_max+1), dtype=DTYPE)\n\n    cdef double[:] g0_ref_ary = np.empty(len(sb1_ref_ary), dtype=DTYPE)\n    cdef double[:] g1_ref_ary_1 = np.empty(len(sb1_ref_ary), dtype=DTYPE)\n    cdef double[:] g1_ref_ary_2 = np.empty(len(sb2_ref_ary), dtype=DTYPE)\n    cdef double[:] g2_ref_ary = np.empty(len(sb2_ref_ary), dtype=DTYPE)\n\n    cdef double[:] g0_int_ary = np.empty(npix_roi, dtype=DTYPE)\n    cdef double[:] g1_int_ary_1 = np.empty(npix_roi, dtype=DTYPE)\n    cdef double[:] g1_int_ary_2 = np.empty(npix_roi, dtype=DTYPE)\n    cdef double[:] g2_int_ary = np.empty(npix_roi, dtype=DTYPE)\n\n    cdef double[::1] ffac_ary = np.empty(npix_roi, dtype=DTYPE)\n    cdef double[:] sb1_eff_ary = np.empty (npix_roi, dtype=DTYPE)\n    cdef double[:] sb2_eff_ary = np.empty (npix_roi, dtype=DTYPE)\n    cdef double[:,::1] sb1_eff_mat = np.zeros((npix_roi, k_max+1), dtype=DTYPE)\n    cdef double[:,::1] sb2_eff_mat = np.zeros((npix_roi, k_max+1), dtype=DTYPE)\n\n    cdef bool ft_cond = False\n\n    if len(ft_compressed) != 0:\n        ft_cond = True\n        # print(\"Nontrivial flux template provided\")\n        # assert(0 not in ft_compressed), \\\n        #     \"Flux template contains one or more 0's!\"       \n    # if ft_cond != True:\n    #     print(\"No nontrivial flux templates\")\n\n    for f_index in range(len(f_ary)):\n        f2 = float(f_ary[f_index])\n        df_rho_div_f2 = df_rho_div_f_ary[f_index]\n\n        if ft_cond:\n            for i in range(len(sb1_ref_ary)):\n                g0_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n1, sb1_ref_ary[i] * f2)\n                g0_ref_mat[i,:] = g0_ary_f\n                g1_ary_f_1 = igf.incgamma_up_fct_ary(k_max, 1. - n2, sb1_ref_ary[i] * f2)\n                g1_ref_mat_1[i,:] = g1_ary_f_1\n\n            # g1_ary_f_2 = igf.incgamma_up_fct_ary(k_max, 1. - n2, sb2 * f2)\n            # g2_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n3, sb2 * f2)\n            for j in range(len(sb2_ref_ary)):\n                g1_ary_f_2 = igf.incgamma_up_fct_ary(k_max, 1. - n2, sb2_ref_ary[j] * f2) \n                g1_ref_mat_2[j,:] = g1_ary_f_2\n                g2_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n3, sb2_ref_ary[j] * f2)\n                g2_ref_mat[j,:] = g2_ary_f\n            \n            for p in range(npix_roi):\n                for k in range(k_max):\n                    if k <= data[p]+1:\n                        sb1_eff_mat[p,k] = ft_compressed[p] * sb1\n                        sb2_eff_mat[p,k] = ft_compressed[p] * sb2\n\n            for k in range(k_max):\n                g0_ref_ary = g0_ref_mat[:,k]\n                g1_ref_ary_1 = g1_ref_mat_1[:,k]\n                g1_ref_ary_2 = g1_ref_mat_2[:,k]\n                g2_ref_ary = g2_ref_mat[:,k]\n                sb1_eff_ary = sb1_eff_mat[:,k]\n                sb2_eff_ary = sb2_eff_mat[:,k]\n\n                minval0 = findmin.minval(g0_ref_ary)\n                if minval0 <= 0:\n                    g0_int_ary = interp.logloginterp1d(sb1_ref_ary, g0_ref_ary, sb1_eff_ary, offset = 1-minval0) \n                else:\n                    g0_int_ary = interp.logloginterp1d(sb1_ref_ary, g0_ref_ary, sb1_eff_ary)\n                g0_mat_f[:,k] = g0_int_ary\n\n                minval1_1 = findmin.minval(g1_ref_ary_1)\n                if minval1_1 <= 0:\n                    g1_int_ary_1 = interp.logloginterp1d(sb1_ref_ary, g1_ref_ary_1, sb1_eff_ary, offset = 1-minval1_1) \n                else:\n                    g1_int_ary_1 = interp.logloginterp1d(sb1_ref_ary, g1_ref_ary_1, sb1_eff_ary)\n                g1_mat_f_1[:,k] = g1_int_ary_1\n\n                minval1_2 = findmin.minval(g1_ref_ary_2)\n                if minval1_2 <= 0:\n                    g1_int_ary_2 = interp.logloginterp1d(sb2_ref_ary, g1_ref_ary_2, sb2_eff_ary, offset = 1-minval1_2) \n                else:\n                    g1_int_ary_2 = interp.logloginterp1d(sb2_ref_ary, g1_ref_ary_2, sb2_eff_ary)\n                g1_mat_f_2[:,k] = g1_int_ary_2\n\n                minval2 = findmin.minval(g2_ref_ary)\n                if minval2 <= 0:\n                    g2_int_ary = interp.logloginterp1d(sb2_ref_ary, g2_ref_ary, sb2_eff_ary, offset = 1-minval2) \n                else:\n                    g2_int_ary = interp.logloginterp1d(sb2_ref_ary, g2_ref_ary, sb2_eff_ary)\n                g2_mat_f[:,k] = g2_int_ary\n            # print(\"Finished interpolating!\")\n            # print(\"Using exact values!\")\n            for p in range(npix_roi):\n                a_ps = float(theta[0])/ft_compressed[p]\n                sb1 = ft_compressed[p] * float(theta[4])\n                sb2 = ft_compressed[p] * float(theta[5])\n             \n                second1_x_m_sum_ary = (1 - pow(sb2/sb1, 1-n2))\n                second2_x_m_sum_ary = pow(sb2/sb1, 1-n2)\n\n                pref0_x_m_ary = pow(sb1 * f2, n1)\n                pref1_x_m_ary = pref0_x_m_ary * pow(sb1 * f2, n2 - n1)\n                pref2_x_m_ary = pref1_x_m_ary * pow(sb2 * f2, n3 - n2)\n\n                # g0_int_ary = igf.incgamma_up_fct_ary(int(data[p]+1), 1. - n1, sb1 * f2)\n                # g1_int_ary_1 = igf.incgamma_up_fct_ary(int(data[p]+1), 1. - n2, sb1 * f2)\n                # g1_int_ary_2 = igf.incgamma_up_fct_ary(int(data[p]+1), 1. - n2, sb2 * f2)\n                # g2_int_ary = igf.incgamma_lo_fct_ary(int(data[p]+1), 1. - n3, sb2 * f2)\n\n                x_m_sum_f = a_ps * (sb1 * f2) * (first0_x_m_sum_ary*second0_x_m_sum_ary\n                            + first1_x_m_sum_ary*second1_x_m_sum_ary\n                            + first2_x_m_sum_ary*second2_x_m_sum_ary) \\\n                            * npt_compressed[p]\n                x_m_sum[p] += df_rho_div_f2 * x_m_sum_f\n\n                for k in range(data[p]+1):\n                    # x_m_ary_f = a_ps * (pref0_x_m_ary * g0_int_ary[k] \n                    #             + pref1_x_m_ary * (g1_int_ary_2[k] - g1_int_ary_1[k])\n                    #             + pref2_x_m_ary * g2_int_ary[k]) * npt_compressed[p]\n                    x_m_ary_f = a_ps * (pref0_x_m_ary * g0_mat_f[p,k] \n                                + pref1_x_m_ary * (g1_mat_f_2[p,k] - g1_mat_f_1[p,k])\n                                + pref2_x_m_ary * g2_mat_f[p,k]) * npt_compressed[p]\n                    # print('x_m=',x_m_ary_f)\n                    x_m_ary[p,k] += df_rho_div_f2 * x_m_ary_f\n\n        else:\n            g0_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n1, sb1 * f2)\n            g1_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n2, sb2 * f2) \\\n                       - igf.incgamma_up_fct_ary(k_max, 1. - n2, sb1 * f2)\n            g2_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n3, sb2 * f2)\n\n            pref0_x_m_ary = pow(sb1 * f2, n1)\n            pref1_x_m_ary = pref0_x_m_ary * pow(sb1 * f2, n2 - n1)\n            pref2_x_m_ary = pref1_x_m_ary * pow(sb2 * f2, n3 - n2)\n\n            for p in range(npix_roi):\n                x_m_sum_f = a_ps * (sb1 * f2) * (first0_x_m_sum_ary*second0_x_m_sum_ary\n                            + first1_x_m_sum_ary*second1_x_m_sum_ary\n                            + first2_x_m_sum_ary*second2_x_m_sum_ary) \\\n                            * npt_compressed[p]\n                x_m_sum[p] += df_rho_div_f2 * x_m_sum_f\n\n                for k in range(data[p]+1):\n                    x_m_ary_f = a_ps * (pref0_x_m_ary * g0_ary_f[k] + pref1_x_m_ary\n                                * g1_ary_f[k] + pref2_x_m_ary * g2_ary_f[k]) \\\n                                * npt_compressed[p]\n                    x_m_ary[p,k] += df_rho_div_f2 * x_m_ary_f\n\n    x_m_sum = np.asarray(x_m_sum) - np.asarray(x_m_ary)[:,0] \n\n    return np.asarray(x_m_ary), np.asarray(x_m_sum)\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.cdivision(True)\n@cython.initializedcheck(False)\ndef return_xs_2break_orig(double[::1] theta, double[::1] f_ary, \n                     double[::1] df_rho_div_f_ary, double[::1] ft_compressed, double[::1] npt_compressed,\n                     int[::1] data):\n    \"\"\" Dedicated calculation of x_m and x_m_sum for 2 breaks \n    \"\"\"\n\n    cdef float a_ps = float(theta[0])\n    cdef float n1 = float(theta[1])\n    cdef float n2 = float(theta[2])\n    cdef float n3 = float(theta[3])\n    cdef float sb1 = float(theta[4])\n    cdef float sb2 = float(theta[5])\n\n    cdef int k_max = int(np.max(data) + 1)\n    cdef int npix_roi = len(npt_compressed)\n\n    cdef double[:,::1] x_m_ary = np.zeros(shape=(npix_roi,k_max + 1))\n    cdef double[::1] x_m_sum = np.zeros(npix_roi)\n\n    cdef double[::1] g0_ary_f = np.zeros(k_max + 1)\n    cdef double[::1] g1_ary_f = np.zeros(k_max + 1)\n    cdef double[::1] g2_ary_f = np.zeros(k_max + 1)\n\n    cdef double f2, df_rho_div_f2\n    cdef double pref0_x_m_ary, pref1_x_m_ary, pref2_x_m_ary\n    cdef double first0_x_m_sum_ary, first1_x_m_sum_ary, first2_x_m_sum_ary\n    cdef double second0_x_m_sum_ary, second1_x_m_sum_ary, second2_x_m_sum_ary\n    cdef double x_m_sum_f, x_m_ary_f\n\n    cdef Py_ssize_t f_index, p, k\n\n    first0_x_m_sum_ary = 1/(1-n1)\n    first1_x_m_sum_ary = 1/(1-n2)\n    first2_x_m_sum_ary = 1/(1-n3)\n\n    second0_x_m_sum_ary = -1.0\n    second1_x_m_sum_ary = (1 - pow(sb2/sb1, 1-n2))\n    second2_x_m_sum_ary = pow(sb2/sb1, 1-n2)\n\n    for f_index in range(len(f_ary)):\n        f2 = float(f_ary[f_index])\n        df_rho_div_f2 = df_rho_div_f_ary[f_index]\n        \n        g0_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n1, sb1 * f2)\n        g1_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n2, sb2 * f2) \\\n                   - igf.incgamma_up_fct_ary(k_max, 1. - n2, sb1 * f2)\n        g2_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n3, sb2 * f2)\n\n        pref0_x_m_ary = pow(sb1 * f2, n1)\n        pref1_x_m_ary = pref0_x_m_ary * pow(sb1 * f2, n2 - n1)\n        pref2_x_m_ary = pref1_x_m_ary * pow(sb2 * f2, n3 - n2)\n\n        for p in range(npix_roi):\n            x_m_sum_f = a_ps * (sb1 * f2) * (first0_x_m_sum_ary*second0_x_m_sum_ary\n                        + first1_x_m_sum_ary*second1_x_m_sum_ary\n                        + first2_x_m_sum_ary*second2_x_m_sum_ary) \\\n                        * npt_compressed[p]\n            x_m_sum[p] += df_rho_div_f2 * x_m_sum_f\n\n            for k in range(data[p]+1):\n                x_m_ary_f = a_ps * (pref0_x_m_ary * g0_ary_f[k] + pref1_x_m_ary\n                            * g1_ary_f[k] + pref2_x_m_ary * g2_ary_f[k]) \\\n                            * npt_compressed[p]\n                x_m_ary[p,k] += df_rho_div_f2 * x_m_ary_f\n\n    x_m_sum = np.asarray(x_m_sum) - np.asarray(x_m_ary)[:,0] \n\n    return np.asarray(x_m_ary), np.asarray(x_m_sum)\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.cdivision(True)\n@cython.initializedcheck(False)\ndef return_xs_3break(double[::1] theta, double[::1] f_ary, double[::1] df_rho_div_f_ary, \n                     double[::1] ft_compressed, double[::1] npt_compressed, int[::1] data):\n    \"\"\" Dedicated calculation of x_m and x_m_sum for 3 breaks \n    \"\"\"\n\n    cdef float a_ps = float(theta[0])\n    cdef float n1 = float(theta[1])\n    cdef float n2 = float(theta[2])\n    cdef float n3 = float(theta[3])\n    cdef float n4 = float(theta[4])\n    cdef float sb1 = float(theta[5])\n    cdef float sb2 = float(theta[6])\n    cdef float sb3 = float(theta[7])\n\n    cdef int k_max = int(np.max(data) + 1)\n    cdef int npix_roi = len(npt_compressed)\n\n    cdef double[:,::1] x_m_ary = np.zeros((npix_roi,k_max + 1))\n    cdef double[::1] x_m_sum = np.zeros(npix_roi)\n\n    cdef double[::1] g0_ary_f = np.zeros(k_max + 1)\n    cdef double[::1] g1_ary_f = np.zeros(k_max + 1)\n    cdef double[::1] g2_ary_f = np.zeros(k_max + 1)\n    cdef double[::1] g3_ary_f = np.zeros(k_max + 1)\n\n    cdef double f2, df_rho_div_f2, fluxfac\n    cdef double pref0_x_m_ary, pref1_x_m_ary, pref2_x_m_ary, pref3_x_m_ary\n    cdef double first0_x_m_sum_ary, first1_x_m_sum_ary, first2_x_m_sum_ary\n    cdef double first3_x_m_sum_ary\n    cdef double second0_x_m_sum_ary, second1_x_m_sum_ary, second2_x_m_sum_ary\n    cdef double second3_x_m_sum_ary\n    cdef double x_m_sum_f, x_m_ary_f\n\n    cdef Py_ssize_t f_index, p, k\n\n    first0_x_m_sum_ary = 1/(1-n1)\n    first1_x_m_sum_ary = 1/(1-n2)\n    first2_x_m_sum_ary = 1/(1-n3)\n    first3_x_m_sum_ary = 1/(1-n4)\n\n    second0_x_m_sum_ary = -1.0\n    second1_x_m_sum_ary = (1 - pow(sb2/sb1, 1-n2))\n    second2_x_m_sum_ary = pow(sb2/sb1, 1-n2) * (1 - pow(sb3/sb2, 1-n3))\n    second3_x_m_sum_ary = pow(sb2/sb1, 1-n2) * pow(sb3/sb2, 1-n3)\n\n    for f_index in range(len(f_ary)):\n        f2 = float(f_ary[f_index])\n        df_rho_div_f2 = df_rho_div_f_ary[f_index]\n        g0_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n1, sb1 * f2)\n        g1_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n2, sb2 * f2) \\\n                   - igf.incgamma_up_fct_ary(k_max, 1. - n2, sb1 * f2)\n        g2_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n3, sb3 * f2) \\\n                   - igf.incgamma_up_fct_ary(k_max, 1. - n3, sb2 * f2)\n        g3_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n4, sb3 * f2)\n\n        for p in range(npix_roi):\n            fluxfac = ft_compressed[p]\n            if fluxfac != 1:\n                g0_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n1, sb1 * fluxfac * f2)\n                g1_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n2, sb2 * fluxfac * f2) \\\n                           - igf.incgamma_up_fct_ary(k_max, 1. - n2, sb1 * fluxfac * f2)\n                g2_ary_f = igf.incgamma_up_fct_ary(k_max, 1. - n3, sb3 * fluxfac * f2) \\\n                           - igf.incgamma_up_fct_ary(k_max, 1. - n3, sb2 * fluxfac * f2)\n                g3_ary_f = igf.incgamma_lo_fct_ary(k_max, 1. - n4, sb3 * fluxfac * f2)\n\n            pref0_x_m_ary = pow(sb1 * fluxfac * f2, n1)\n            pref1_x_m_ary = pref0_x_m_ary * pow(sb1 * fluxfac * f2, n2 - n1)\n            pref2_x_m_ary = pref1_x_m_ary * pow(sb2 * fluxfac * f2, n3 - n2)\n            pref3_x_m_ary = pref2_x_m_ary * pow(sb3 * fluxfac * f2, n4 - n3)\n\n            x_m_sum_f = (a_ps/fluxfac * sb1 * fluxfac * f2) * (first0_x_m_sum_ary*second0_x_m_sum_ary\n                        + first1_x_m_sum_ary*second1_x_m_sum_ary\n                        + first2_x_m_sum_ary*second2_x_m_sum_ary\n                        + first3_x_m_sum_ary*second3_x_m_sum_ary) \\\n                        * npt_compressed[p]\n            x_m_sum[p] += df_rho_div_f2 * x_m_sum_f\n\n            for k in range(data[p]+1):\n                x_m_ary_f = a_ps/fluxfac * (pref0_x_m_ary*g0_ary_f[k]\n                            + pref1_x_m_ary*g1_ary_f[k]\n                            + pref2_x_m_ary*g2_ary_f[k]\n                            + pref3_x_m_ary*g3_ary_f[k]) \\\n                            * npt_compressed[p]\n                x_m_ary[p,k] += df_rho_div_f2 * x_m_ary_f\n\n    x_m_sum = np.asarray(x_m_sum) - np.asarray(x_m_ary)[:,0] \n\n    return np.asarray(x_m_ary), np.asarray(x_m_sum)\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.cdivision(True)\n@cython.initializedcheck(False)\ndef return_xs_lbreak(double[::1] theta, int n_break, double[::1] f_ary, \n                     double[::1] df_rho_div_f_ary, double[::1] npt_compressed,\n                     int[::1] data):\n    \"\"\" General calculation of x_m and x_m_sum for l breaks \n    \"\"\"\n\n    cdef float a_ps = float(theta[0])\n    cdef double[::1] n_ary = theta[1:n_break + 2]\n    cdef double[::1] sb_ary = theta[n_break + 2: 2*n_break + 2]\n\n    cdef int k_max = int(np.max(data) + 1)\n    cdef int npix_roi = len(npt_compressed)\n\n    cdef double[:,::1] x_m_ary = np.zeros((npix_roi, k_max + 1))\n    cdef double[::1] x_m_sum = np.zeros(npix_roi)\n\n    cdef double[:,::1] g_ary_f_ary = np.zeros((n_break + 1, k_max + 1))\n    cdef double[::1] pref_x_m_ary = np.zeros(n_break + 1)\n\n    cdef double[::1] first_x_m_sum_ary = np.zeros(n_break + 1)\n    cdef double[::1] second_x_m_sum_ary = np.zeros(n_break + 1)\n\n    cdef double[::1] gamma_ary = np.zeros(k_max + 1)\n\n    cdef double f2, df_rho_div_f2\n\n    cdef double[::1] sbpow_ary = np.zeros(n_break + 1)\n\n    cdef double dp = 0.0, dp_sum = 0\n\n    cdef double x_m_sum_f, x_m_ary_f\n\n    cdef Py_ssize_t f_index, p, k, m, i, j\n\n    # Get (Sb[j]/Sb[j+1])**(-n[j]) to avoid having to call pow() repeatedly \n    for j in range(n_break + 1):\n        sbpow_ary[j] = pow(sb_ary[j] / sb_ary[j-1], - n_ary[j])\n\n    # x_m_sum factors 1\n    first_x_m_sum_ary[0] = -1. \n\n    for i in range(1, n_break + 1):\n        first_x_m_sum_ary[i] = first_x_m_sum_ary[0]\n        for j in range(1, i):\n            first_x_m_sum_ary[i] *= pow(sb_ary[j] / sb_ary[j-1], - n_ary[j])\n        first_x_m_sum_ary[i] *= sb_ary[i-1] * ( - 1 + sbpow_ary[i]\n                                * (sb_ary[i] / sb_ary[i-1])) / sb_ary[0]\n\n    first_x_m_sum_ary[n_break] = 1.\n    for j in range(1, n_break):\n        first_x_m_sum_ary[n_break] *= sbpow_ary[j]\n    first_x_m_sum_ary[n_break]  *=  sb_ary[n_break-1] / sb_ary[0]\n\n    # x_m_sum factors 2\n    for i in range(n_break+1):\n        second_x_m_sum_ary[i] = 1/(1 - n_ary[i])\n\n    for f_index in range(len(f_ary)): # PSF loop\n        f2 = float(f_ary[f_index])\n        df_rho_div_f2 = df_rho_div_f_ary[f_index]\n\n        # Terms involving incomplete gamma functions in x_m\n        gamma_ary = igf.incgamma_up_fct_ary(k_max, 1.-n_ary[0], sb_ary[0] * f2)\n        for m in range(k_max + 1):\n            g_ary_f_ary[0][m] = gamma_ary[m]\n\n        for i in range(1, n_break):\n            gamma_ary = igf.incgamma_up_fct_ary(k_max, 1.-n_ary[i], sb_ary[i] * f2) \\\n                        - igf.incgamma_up_fct_ary(k_max, 1.-n_ary[i], sb_ary[i-1] * f2)\n            for m in range(k_max + 1):\n                g_ary_f_ary[i][m] = gamma_ary[m]\n\n        gamma_ary = igf.incgamma_lo_fct_ary(k_max, 1.-n_ary[n_break], sb_ary[n_break-1] * f2)\n        for m in range(k_max + 1):\n            g_ary_f_ary[n_break][m] = gamma_ary[m]\n\n        # Terms not involving incomplete gamma functions in x_m\n        pref_x_m_ary[0] = pow(sb_ary[0] * f2, n_ary[0])\n\n        for i in range(1, n_break + 1):\n            pref_x_m_ary[i] = pref_x_m_ary[i-1] * pow(sb_ary[i-1] * f2, n_ary[i]-n_ary[i-1])\n\n        dp_sum = 0.0\n        for i in range(0, n_break+1):\n            dp_sum += first_x_m_sum_ary[i] *second_x_m_sum_ary[i]\n\n        for p in range(npix_roi):\n            x_m_sum_f = a_ps * sb_ary[0] * f2 * dp_sum * npt_compressed[p]\n            x_m_sum[p] += df_rho_div_f2 * x_m_sum_f\n            \n            for k in range(data[p]+1):\n                dp = 0.0\n                for i in range(0, n_break+1):\n                    dp += pref_x_m_ary[i] * g_ary_f_ary[i][k]\n\n                x_m_ary_f = a_ps * dp * npt_compressed[p]\n                x_m_ary[p,k] += df_rho_div_f2 * x_m_ary_f\n\n    x_m_sum = np.asarray(x_m_sum) - np.asarray(x_m_ary)[:,0] \n\n    return np.asarray(x_m_ary), np.asarray(x_m_sum)\n\n",
			"file": "x_m.pyx",
			"file_size": 29173,
			"file_write_time": 131398743360000000,
			"settings":
			{
				"buffer_size": 29177,
				"line_ending": "Unix"
			}
		},
		{
			"file": "nptfit.py",
			"settings":
			{
				"buffer_size": 1893,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "incgamma_fct.pyx",
			"settings":
			{
				"buffer_size": 1584,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "###############################################################################\n# dnds_analysis.py\n###############################################################################\n#\n# Analyze results of a non-Poissonian template fit. Code to produce:\n#\n# - Template intensities and confidence intervals\n# - Source count distributions\n# - Intensity fractions\n# - Triangle plots and log-evidences\n#\n# NB: code default to assumption analysis is done on a HEALPix map, if this\n# is not the case, must insert a pixarea at initialization.\n#\n###############################################################################\n\nimport numpy as np\nimport numpy.ma as ma\nimport matplotlib.pyplot as plt\nimport corner\n\n\nclass Analysis:\n    \"\"\" Class to analyze results of an NPTF.\n\n        :param nptf: an instance of nptfit.NPTF, where load_scan() has been performed\n        :param mask: if analysis is to be performed in a different ROI to the run, insert\n        the analysis mask here\n        :param pixarea: if using a non-HEALPix map, insert the area of a pixel (in sr)\n    \"\"\"\n\n    def __init__(self, nptf, mask=None, pixarea=0.):\n\n        self.nptf = nptf\n        # Default to HEALPix map\n        if pixarea == 0.:\n            self.pixarea = 4*np.pi/self.nptf.npix\n        else:\n            self.pixarea = pixarea\n        if mask is None:\n            self.mask_total = self.nptf.mask_total\n        else:\n            self.mask_total = mask\n        self.mask_compress_data()\n\n    def return_intensity_arrays_non_poiss(self, comp, smin=0.01, smax=10000,\n                                          nsteps=10000, counts=False):\n        \"\"\" Return intensity quantiles of a non-Poissonian template\n\n            :param comp: key of non-Poissonian template\n            :param smin: minimum count to \"integrate\" dnds from\n            :param smax: maximum count to \"integrate\" dnds to\n            :param nsteps: number of count bins in sum approximation of integral\n            :param counts: whether to return counts (or intensities, by default)\n        \"\"\"\n\n        template = self.nptf.templates_dict_nested[comp]['template']\n        template_masked_compressed = self.mask_and_compress(\n                                            template, self.mask_total)\n\n        # If intensity, convert from counts to counts/cm^2/s/sr\n        if counts:\n            self.template_sum = np.sum(template_masked_compressed)\n        else:\n            self.template_sum = np.mean(template_masked_compressed /\n                                        self.exp_masked_compressed /\n                                        self.pixarea)\n\n        self.sarray = 10**np.linspace(np.log10(smin), np.log10(smax), nsteps)\n        self.ds = [self.sarray[i+1]-self.sarray[i]\n                   for i in range(len(self.sarray)-1)]\n        self.ds = np.array(self.ds + [self.ds[-1]])\n\n        # Get NPT intensity arrays. These are calculated as\n        # \\int(dS*S*dN/dS). Note that the APS parameter is a\n        # rescaling of the counts, which is why to get the\n        # intensity this is multiplied by the total counts\n        self.intensity_array_non_poiss = \\\n            list(map(lambda sample:\n                     np.sum(self.template_sum *\n                            self.dnds(comp, sample, self.sarray) *\n                            self.sarray*self.ds), self.nptf.samples))\n\n        return self.intensity_array_non_poiss\n\n    def return_intensity_arrays_poiss(self, comp, counts=False):\n        \"\"\" Return intensity arrays of a Poissonian template\n\n            :param comp: key of Poissonian template\n            :param counts: whether to return counts (or intensities, by default)\n        \"\"\"\n\n        template = self.nptf.templates_dict_nested[comp]['template']\n        template_masked_compressed = self.mask_and_compress(\n                                            template, self.mask_total)\n\n        # If intensity, convert from counts to counts/cm^2/s/sr\n        if counts:\n            self.template_sum = np.sum(template_masked_compressed)\n        else:\n            self.template_sum = np.mean(template_masked_compressed /\n                                        self.exp_masked_compressed /\n                                        self.pixarea)\n\n        # Get PT intensities by scaling the compressed mask intensity\n        # by the relevant normalizations from chains\n        self.intensity_array_poiss = \\\n            list(map(lambda sample: self.template_sum *\n                     self.return_poiss_samples(comp, sample),\n                     self.nptf.samples))\n\n        return self.intensity_array_poiss\n\n    def return_non_poiss_samples(self, comp, sample):\n        \"\"\" Return non-Poissonian samples corrected for log priors\n        \"\"\"\n\n        # Load all NPT models (stored after PT models)\n        self.model_decompression_non_poiss = \\\n            np.array(self.nptf.model_decompression_key[self.nptf.n_poiss:])\n\n        model_where = \\\n            np.where(self.model_decompression_non_poiss[:, 0] == comp)[0] \\\n            + self.nptf.n_poiss\n\n        is_log_prior = \\\n            np.array(self.nptf.model_decompression_key)[model_where][:, 1]\n\n        is_log_prior = list(is_log_prior == 'True')\n\n        samples_model_not_log = \\\n            self.log_to_normal(np.array(sample)[model_where], is_log_prior)\n\n        return samples_model_not_log\n\n    def return_poiss_samples(self, comp, sample):\n        \"\"\" Return Poissonian samples corrected for log priors\n        \"\"\"\n\n        # Load all PT models\n        self.model_decompression_poiss = \\\n            np.array(self.nptf.model_decompression_key[:self.nptf.n_poiss])\n\n        model_where = \\\n            np.where(self.model_decompression_poiss[:, 0] == comp)[0]\n\n        is_log_prior = \\\n            np.array(self.model_decompression_poiss)[model_where][0][1]\n\n        samples_model_not_log = self.log_to_normal(\n            np.array(sample)[model_where], [is_log_prior == 'True'])[0]\n\n        return samples_model_not_log\n\n    def return_dndf_arrays(self, comp, flux):\n        \"\"\" Calculate and return array of dN/dF values for the template comp\n            and the given array of flux values (in counts/cm^2/s)\n        \"\"\"\n\n        template = self.nptf.templates_dict_nested[comp]['template']\n        template_masked_compressed = \\\n            self.mask_and_compress(template, self.mask_total)\n\n        self.template_sum = np.sum(template_masked_compressed)\n\n        # Rescaling factor to convert dN/dS to [(counts/cm^2 /s)^-2 /deg^2]\n        # Note that self.area_mask has units deg^2.\n        rf = self.template_sum*self.exp_masked_mean/self.area_mask\n\n        # Get counts from flux\n        s = np.array([flux])*self.exp_masked_mean\n\n        return rf*np.array([self.dnds(comp, sample, s)[0]\n                           for sample in self.nptf.samples])\n\n    def calculate_dndf_arrays(self, comp, smin=0.01, smax=1000, nsteps=1000,\n                              qs=[0.16, 0.5, 0.84]):\n        \"\"\" Calculate dnds for specified quantiles\n        \"\"\"\n\n        template = self.nptf.templates_dict_nested[comp]['template']\n        template_masked_compressed = \\\n            self.mask_and_compress(template, self.mask_total)\n\n        self.template_sum = np.sum(template_masked_compressed)\n\n        self.sarray = 10**np.linspace(np.log10(smin), np.log10(smax), nsteps)\n\n        self.flux_array = self.sarray/self.exp_masked_mean\n\n        self.data_array = np.array([self.dnds(comp, sample, self.sarray)\n                                   for sample in self.nptf.samples])\n\n        # Rescaling factor to convert dN/dS to [(ph /cm^2 /s)^-2 /deg^2]\n        # Note that self.area_mask has units deg^2.\n        rf = self.template_sum*self.exp_masked_mean/self.area_mask\n\n        self.qArray = [corner.quantile(self.data_array[::, i], qs)\n                       for i in range(len(self.sarray))]\n\n        self.qmean = rf*np.array([np.mean(self.data_array[::, i])\n                                  for i in range(len(self.sarray))])\n\n        self.qlow = rf*np.array([q[0] for q in self.qArray])\n        self.qmid = rf*np.array([q[1] for q in self.qArray])\n        self.qhigh = rf*np.array([q[2] for q in self.qArray])\n\n    def plot_source_count_band(self, comp, smin=0.01, smax=1000, nsteps=1000,\n                               qs=[0.16, 0.5, 0.84], spow=0, *args, **kwargs):\n        \"\"\" Calculate and plot median source count function\n\n            :param comp: key of non-Poissonian template\n            :param smin: minimum count to plot\n            :param smax: maximum count to plot\n            :param nsteps: binning in counts s\n            :param qs: source count quartles to plot\n            :param spow: plotting s**spow*dn/ds\n            **kwargs: plotting options\n        \"\"\"\n\n        self.calculate_dndf_arrays(comp, smin=smin, smax=smax,\n                                   nsteps=nsteps, qs=qs)\n        plt.fill_between(self.flux_array, self.flux_array**spow*self.qlow,\n                         self.flux_array**spow*self.qhigh, *args, **kwargs)\n\n    def plot_source_count_median(self, comp, smin=0.01, smax=1000, nsteps=1000,\n                                 spow=0, qs=[0.16, 0.5, 0.84], *args, **kwargs):\n        \"\"\" Calculate and plot median source count function\n        \"\"\"\n\n        self.calculate_dndf_arrays(comp, smin=smin, smax=smax,\n                                   nsteps=nsteps, qs=qs)\n        plt.plot(self.flux_array, self.flux_array**spow*self.qmid,\n                 *args, **kwargs)\n\n    def plot_intensity_fraction_non_poiss(self, comp, smin=0.00001, smax=1000,\n                                          nsteps=1000, qs=[0.16, 0.5, 0.84],\n                                          bins=50, color='blue',\n                                          ls_vert='dashed', *args, **kwargs):\n        \"\"\" Plot flux fraction of a non-Poissonian template\n\n            :param bins: flux fraction bins\n            :param color_vert: colour of vertical quartile lines\n            :param ls_vert: matplotlib linestyle of vertical quartile lines\n            **kwargs: plotting options\n        \"\"\"\n\n        flux_fraction_array_non_poiss = \\\n            np.array(self.return_intensity_arrays_non_poiss(comp, smin=smin,\n                     smax=smax, nsteps=nsteps, counts=True))/self.total_counts\n\n        frac_hist_comp, bin_edges_comp = \\\n            np.histogram(100*np.array(flux_fraction_array_non_poiss), bins=bins,\n                         range=(0, 100))\n\n        qs_comp = \\\n            corner.quantile(100*np.array(flux_fraction_array_non_poiss), qs)\n\n        plt.plot(bin_edges_comp[:-1],\n                 frac_hist_comp/float(sum(frac_hist_comp)),\n                 color=color, *args, **kwargs)\n\n        for q in qs_comp:\n            plt.axvline(q, ls=ls_vert, color=color)\n        self.qs_comp = qs_comp\n\n    def plot_intensity_fraction_poiss(self, comp, qs=[0.16, 0.5, 0.84], bins=50,\n                                      color='blue', ls_vert='dashed',\n                                      *args, **kwargs):\n        \"\"\" Plot flux fraction of non-Poissonian component\n        \"\"\"\n\n        flux_fraction_array_poiss = \\\n            np.array(self.return_intensity_arrays_poiss(comp, counts=True))\\\n            / self.total_counts\n\n        frac_hist_comp, bin_edges_comp = \\\n            np.histogram(100*np.array(flux_fraction_array_poiss), bins=bins,\n                         range=(0, 100))\n\n        qs_comp = corner.quantile(100*np.array(flux_fraction_array_poiss), qs)\n\n        plt.plot(bin_edges_comp[:-1],\n                 frac_hist_comp/float(sum(frac_hist_comp)), \n                 color=color, *args, **kwargs)\n\n        for q in qs_comp:\n            plt.axvline(q, ls=ls_vert, color=color)\n        self.qs_comp = qs_comp\n\n    def return_poiss_parameter_posteriors(self, comp):\n        \"\"\" Return posterior samples corresponding to individual parameters.\n\n            :return: sample normalization values\n        \"\"\"\n\n        self.samples_reduced_ary = [self.return_poiss_samples(comp, sample)\n                                    for sample in self.nptf.samples]\n        return self.samples_reduced_ary\n\n    def return_non_poiss_parameter_posteriors(self, comp):\n        \"\"\" Return posterior samples corresponding to individual parameters.\n\n            :return: sample non-Poissonian posterior values values, list with\n            self.aps_ary: sample normalization values\n            self.n_ary_ary: sampled slopes, each sub-array corresponding to\n                samples for that slope (highest to lowest).\n            self.sb_ary_ary: sampled breaks, each sub-array corresponding to\n                samples for that break (highest to lowest).\n        \"\"\"\n\n        self.samples_reduced_ary = [self.return_non_poiss_samples(comp, sample)\n                                    for sample in self.nptf.samples]\n        self.samples_reduced_param_ary = list(zip(*self.samples_reduced_ary))\n\n        nbreak = int((len(self.samples_reduced_ary[0]) - 2)/2.)\n\n        self.aps_ary = self.samples_reduced_param_ary[0]\n\n        self.n_ary_ary = [[] for _ in range(nbreak+1)]\n        self.sb_ary_ary = [[] for _ in range(nbreak)]\n\n        for i in range(nbreak+1):\n            self.n_ary_ary[i] = self.samples_reduced_param_ary[i+1]\n\n        for i in range(nbreak):\n            self.sb_ary_ary[i] = self.samples_reduced_param_ary[i+nbreak+2]\n\n        return self.aps_ary, self.n_ary_ary, self.sb_ary_ary\n\n    def dnds(self, comp, sample, s):\n        \"\"\" dN/dS values for NPT comp associated with a chain sample\n        \"\"\"\n\n        samples_reduced = self.return_non_poiss_samples(comp, sample)\n\n        nbreak = int((len(samples_reduced) - 2)/2.)\n\n        # Get APS (float) and slopes/breaks (arrays)\n        a_ps, n_ary, sb_ary = samples_reduced[0], samples_reduced[1:nbreak+2], \\\n            samples_reduced[nbreak+2:]\n\n        # If relative breaks, define each break as (except the last one)\n        # the multiplicative factor times the previous break\n        if self.nptf.non_poiss_models[comp]['dnds_model'] \\\n                == 'specify_relative_breaks':\n            for i in reversed(range(len(sb_ary) - 1)):\n                sb_ary[i] = sb_ary[i+1]*sb_ary[i]\n\n        # Determine where the s values fall with respect to the breaks\n        where_vecs = [[] for _ in range(nbreak+1)]\n        where_vecs[0] = np.where(s >= sb_ary[0])[0]\n        for i in range(1, nbreak):\n            where_vecs[i] = np.where((s >= sb_ary[i]) & (s < sb_ary[i-1]))[0]\n        where_vecs[-1] = np.where(s < sb_ary[-1])[0]\n\n        # Calculate dnds values for a broken power law with arbitrary breaks\n        dnds = np.zeros(len(s))\n        dnds[where_vecs[0]] = a_ps*(s[where_vecs[0]]/sb_ary[0])**(-n_ary[0])\n        dnds[where_vecs[1]] = a_ps*(s[where_vecs[1]]/sb_ary[0])**(-n_ary[1])\n\n        for i in range(2, nbreak+1):\n            dnds[where_vecs[i]] = \\\n                a_ps*np.prod([(sb_ary[j+1]/sb_ary[j])**(-n_ary[j+1])\n                              for j in range(0, i-1)]) * \\\n                (s[where_vecs[i]]/sb_ary[i-1])**(-n_ary[i])\n\n        return dnds\n\n    @staticmethod\n    def log_to_normal(array, is_log):\n        \"\"\" Take array and account for the impact of log priors\n        \"\"\"\n\n        array_normal = []\n        for i in range(len(array)):\n            if is_log[i]:\n                array_normal.append(10**array[i])\n            else:\n                array_normal.append(array[i])\n        return array_normal\n\n    @staticmethod\n    def mask_and_compress(the_map, mask):\n        \"\"\" Return compressed version of a map\n        \"\"\"\n\n        map_masked = ma.masked_array(data=the_map, mask=mask)\n        return map_masked.compressed()\n\n    def mask_compress_data(self):\n        \"\"\" Adjust the data and exposure for the mask\n        \"\"\"\n\n        self.data_masked = ma.masked_array(data=self.nptf.count_map,\n                                           mask=self.mask_total)\n        self.data_masked_compressed = self.data_masked.compressed()\n        self.total_counts = float(np.sum(self.data_masked_compressed))\n\n        self.area_mask = np.sum(1-self.mask_total)*self.pixarea *\\\n            (360/(2.*np.pi))**2\n\n        self.exp_masked = ma.masked_array(data=self.nptf.exposure_map,\n                                          mask=self.mask_total)\n        self.exp_masked_compressed = self.exp_masked.compressed()\n        self.exp_masked_mean = np.mean(self.exp_masked_compressed)\n\n    def get_log_evidence(self):\n        \"\"\" Global log-evidence and associated error\n        \"\"\"\n\n        self.lge = self.nptf.s['nested sampling global log-evidence']\n        self.lge_err = self.nptf.s['nested sampling global log-evidence error']\n        return self.lge, self.lge_err\n\n    def make_triangle(self):\n        \"\"\" Make a triangle plot\n        \"\"\"\n\n        corner.corner(self.nptf.samples, labels=self.nptf.params, smooth=1.5,\n                      smooth1d=1, quantiles=[0.16, 0.5, 0.84], show_titles=True,\n                      title_fmt='.2f', title_args={'fontsize': 14},\n                      range=[1 for _ in range(self.nptf.n_params)],\n                      plot_datapoints=False, verbose=False)\n",
			"file": "dnds_analysis.py",
			"file_size": 17056,
			"file_write_time": 131400687280000000,
			"settings":
			{
				"buffer_size": 17056,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 87.0,
		"last_filter": "install pa",
		"selected_items":
		[
			[
				"install pa",
				"Package Control: Install Package"
			],
			[
				"package",
				"Package Control: Install Package"
			]
		],
		"width": 485.0
	},
	"console":
	{
		"height": 126.0,
		"history":
		[
			"install",
			"import urllib.request,os,hashlib; h = 'df21e130d211cfc94d9b0905775a7c0f' + '1e3d39e33b79698005270310898eea76'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)",
			"install",
			"import urllib.request,os,hashlib; h = 'df21e130d211cfc94d9b0905775a7c0f' + '1e3d39e33b79698005270310898eea76'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)",
			"install ",
			"import urllib.request,os,hashlib; h = 'df21e130d211cfc94d9b0905775a7c0f' + '1e3d39e33b79698005270310898eea76'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/Users/laurachang/NPTFit",
		"/Users/laurachang/NPTFit/NPTFit"
	],
	"file_history":
	[
		"/Users/laurachang/NPTFit/NPTFit/create_mask.py",
		"/Users/laurachang/NPTFit/.gitignore",
		"/Users/laurachang/NPTFit/NPTFit/incgamma_fct.c",
		"/Users/laurachang/NPTFit/setup.py",
		"/Users/laurachang/NPTFit/sftp-config.json",
		"/Users/laurachang/NPTFit/NPTFit/incgamma_fct_p.pyx",
		"/Users/laurachang/NPTFit/NPTFit/incgamma_fct.h",
		"/Users/laurachang/Library/Application Support/Sublime Text 3/Packages/User/SFTP.errors.log",
		"/Users/laurachang/NPTFit/NPTFit/x_m.html",
		"/Users/laurachang/NPTFit/NPTFit/incgamma_fct.pyx",
		"/Users/laurachang/NPTFit/NPTFit/pll.pyx",
		"/Users/laurachang/NPTFit/NPTFit/__init__.py",
		"/Users/laurachang/NPTFit/NPTFit/x_m.c",
		"/Users/laurachang/NPTFit/NPTFit/npll.c",
		"/Users/laurachang/NPTFit/NPTFit/psf_correction.py",
		"/Users/laurachang/NPTFit/NPTFit/dnds_analysis.py",
		"/Users/laurachang/NPTFit/NPTFit/npll.pyx",
		"/Users/laurachang/NPTFit/NPTFit/nptf_scan.py",
		"/Users/laurachang/NPTFit/NPTFit/config_maps.py",
		"/Users/laurachang/NPTFit/NPTFit/x_m.pyx",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/Fermi-Subhalos-NPTF.sublime-project",
		"/Users/laurachang/NPTFit/NPTFit/nptfit.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/subhalo_runs/NPTF_Ein_1em25.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/subhalo_runs/NPTF_Ein_1em24.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/subgen.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/subhalo_runs/NPTF_Blazar_2Breaks.py",
		"/Users/laurachang/cosmosis-docker/cosmosis/cosmosis-standard-library/utility/consistency/consistency.py",
		"/Users/laurachang/cosmosis-docker/cosmosis/cosmosis-standard-library/utility/consistency/consistency_interface.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/subhalo_runs/NPTF_Ein_iso_only.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/astrogen.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/particle.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/nfw.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/einasto.py",
		"/Users/laurachang/cosmosis/cosmosis/datablock/generate_sections.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/subhalo_runs/NPTF_Einasto_2Breaks.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/subhalo_runs/NPTF_Runs_3FGL.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/units.py",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/sftp-config.json",
		"/Users/laurachang/Desktop/Princeton/Research/Fermi-Subhalos-NPTF/Fermi-Subhalos-NPTF.sublime-workspace",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_3FGL_noNFW.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_Blazar_2Breaks.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_3FGL_withNFW.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_3FGL_Blazars_Bright.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_3FGL_Blazars_withNFW.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_3FGL_Blazars_noNFW.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/sftp-config.json",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_Runs.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/astrogen.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_Runs_PS.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/notebooks/NPTF_Runs_PS.ipynb",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/notebooks/NPTF_Runs.ipynb",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/particle.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/subhalo_runs/NPTF_Runs_1em25.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/notebooks/Make_Fake_Data_Diffuse_Only.ipynb",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/notebooks/Make_Subhalo_Template.ipynb",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/nfw.py",
		"/Users/laurachang/Desktop/Princeton/Research/Feynman/Fermi_Subhalos_NPTF/NPTF_Runs_1em25.py",
		"/Users/laurachang/Desktop/Princeton/Feynman/From sid/sftp-config.json",
		"/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480955294/feynman/group/hepheno/smsharma/Fermi-Subhalos-NPTF/README.md",
		"/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480953839/feynman/group/hepheno/smsharma/Fermi-Subhalos-NPTF/notebooks/Make_Fake_Data.ipynb",
		"/Users/laurachang/Desktop/Princeton/Feynman/Sid's/sftp-config.json",
		"/Users/laurachang/Desktop/Princeton/Feynman/Fermi_Subhalos_NPTF/sftp-config.json",
		"/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480952460/feynman/group/hepheno/smsharma/Fermi-Subhalos-NPTF/data/subhalo_flux_map_1em24.npy",
		"/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480951384/mydir/group/hepheno/ljchang/Fermi_Subhalos_NPTF/Analyze_Results.ipynb",
		"/private/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480951384/mydir/group/hepheno/ljchang/Fermi_Subhalos_NPTF/nfw.py",
		"/private/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480951384/mydir/group/hepheno/ljchang/Fermi_Subhalos_NPTF/particle.py",
		"/private/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480951384/mydir/group/hepheno/ljchang/Fermi_Subhalos_NPTF/units.py",
		"/private/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480951384/mydir/group/hepheno/ljchang/Fermi_Subhalos_NPTF/subgen.py",
		"/Users/laurachang/Library/Application Support/Sublime Text 3/Packages/User/sftp_servers/mydir",
		"/Users/laurachang/Library/Application Support/Sublime Text 3/Packages/User/sftp_servers/feynman",
		"/var/folders/r3/4pswdz_97nn_06pkwkztmwkc0000gn/T/sublime-sftp-browse-1480903599/feynman/group/hepheno/ljchang/Basics/Basics.ipynb",
		"/Users/laurachang/Downloads/Analyze_Results (1).ipynb",
		"/Users/laurachang/Basics"
	],
	"find":
	{
		"height": 35.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"load_ma",
			"self.n_non_poiss",
			"theta",
			"incomplete ",
			"incomplete gamma",
			"print",
			"ll",
			"exp_masked_compressed",
			"exp_masked_mean",
			"area_mask",
			"template_sum",
			"pixarea",
			"mask_total",
			"area_mask",
			"NPT",
			"compressed",
			"non_poiss",
			"npt",
			"masked_compressed",
			"template_sum",
			"mask_total",
			"area_mask",
			"pixarea",
			"area_mask",
			"dndf",
			"self.ll",
			"load_non_",
			"non_poi",
			"nexp",
			"error",
			"print",
			"error",
			"error while",
			"print",
			"nexp",
			"npll",
			"log_like",
			"nbreak_ary",
			"theta_ps_ary",
			"theta",
			"PT_sum_compressed",
			"npll",
			"theta_ps_ary",
			"theta_ps",
			"make_ll",
			"make_pt_sum_theta",
			"theta",
			"internal",
			"theta",
			"self.masked_compressed_data",
			"npix",
			"interp1d",
			"k_max_ary",
			"ndarray",
			"ft_compres",
			"ft_com",
			"ones",
			"flux_maps",
			"kmax",
			"sb_ary",
			"self.ft_compressed_exp_ary",
			"npll",
			"k_max",
			"f_ary",
			"assert",
			"ft_compressed",
			"compressed_flux_map",
			"self.ft_compressed_exp_ary",
			"self.ft_dist_compressed_exp_ary",
			"self.ft",
			"self.flux",
			"self.NPT_dist_",
			"self.templates_dict_",
			"self.npt",
			"npll",
			"self.templates_dict_nested",
			"flux_maps",
			"pll",
			"x_m",
			"npll",
			"templates_dict_nested",
			"compress",
			"for i in range",
			"self.templates",
			"n_break",
			"print",
			"npll",
			"templates_dict_nested",
			"compressed",
			"templates_dict",
			"data",
			"flux",
			"f_ary",
			"print",
			"flux_template",
			"f_ary",
			"npll",
			"f_ary",
			"fixed",
			"\n        ",
			"print",
			"rho_c",
			"alpha",
			"r_s",
			"scipy",
			"host"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"folder"
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 8,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "npll.pyx",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4260,
						"regions":
						{
						},
						"selection":
						[
							[
								2394,
								2394
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Cython/Cython.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 638.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "pdf_sampler.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1639,
						"regions":
						{
						},
						"selection":
						[
							[
								655,
								655
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "nptf_scan.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 30507,
						"regions":
						{
						},
						"selection":
						[
							[
								24642,
								24647
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 8292.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "config_maps.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 12281,
						"regions":
						{
						},
						"selection":
						[
							[
								6804,
								6804
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 6.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "interp1d.pyx",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6351,
						"regions":
						{
						},
						"selection":
						[
							[
								5270,
								5270
							]
						],
						"settings":
						{
							"auto_name": "import numpy as N",
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Cython/Cython.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "x_m.pyx",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 29177,
						"regions":
						{
						},
						"selection":
						[
							[
								16803,
								16803
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Cython/Cython.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 6250.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "nptfit.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1893,
						"regions":
						{
						},
						"selection":
						[
							[
								1677,
								1679
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "incgamma_fct.pyx",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1584,
						"regions":
						{
						},
						"selection":
						[
							[
								353,
								369
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Cython/Cython.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "dnds_analysis.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 17056,
						"regions":
						{
						},
						"selection":
						[
							[
								14177,
								14177
							]
						],
						"settings":
						{
							"incomplete_sync": null,
							"remote_loading": false,
							"synced": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1875.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 25.0
	},
	"input":
	{
		"height": 31.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.find_results":
	{
		"height": 0.0
	},
	"output.sftp":
	{
		"height": 144.0
	},
	"pinned_build_system": "Packages/Python/Python.sublime-build",
	"project": "nptfit.sublime-project",
	"replace":
	{
		"height": 46.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"",
				".gitignore"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 204.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
